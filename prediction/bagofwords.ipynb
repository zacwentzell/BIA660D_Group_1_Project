{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use bag-of-words and bag-of-bigrams to predict ratings.\n",
    "BIA660D - Group 1: Alec Kulakowski\n",
    "Here we will explore some basic models applied niavely to simple 1-gram and 1,2-gram representations of the text. Based on performance in this regard, we can explore the most successful of the models in detail later, as well as different representations of the corpus, hyperparameter selection, and feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_rating                                          user_text  \\\n0            5  We booked Grand Vin as our brunch location to ...   \n1            4  Sooooo for date night it was his turn to pick ...   \n2            5  Adorable little wine bar with outdoor seating ...   \n\n  restaurant_name  restaurant_rating  restaurant_price  \\\n0       Grand Vin           3.994975               2.0   \n1       Grand Vin           4.000000               2.0   \n2       Grand Vin           3.994975               2.0   \n\n                             restaurant_type  \n0  ['Wine Bars', 'Italian', 'Cocktail Bars']  \n1  ['Wine Bars', 'Italian', 'Cocktail Bars']  \n2  ['Wine Bars', 'Italian', 'Cocktail Bars']  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../BIA660D_Group_1_Project/eda/hoboken_step1.csv')\n",
    "temp = data.head(3)\n",
    "reviews = data['user_text']\n",
    "ratings = data['user_rating']\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_reviews, test_reviews, train_ratings, test_ratings = train_test_split(reviews, ratings, test_size=0.15, random_state=1)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', binary=True, max_df=0.9, max_features=1000)\n",
    "vectorizer.fit(train_reviews)\n",
    "train_bow = vectorizer.transform(train_reviews)\n",
    "test_bow = vectorizer.transform(test_reviews)\n",
    "bigram_vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', binary=True, max_df=0.9, max_features=1000, ngram_range=(1,2))\n",
    "bigram_vectorizer.fit(train_reviews)\n",
    "train_bigram = bigram_vectorizer.transform(train_reviews)\n",
    "test_bigram = bigram_vectorizer.transform(test_reviews)\n",
    "del data, reviews, ratings, vectorizer, bigram_vectorizer\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we import the relevant data and examine it. We then select the bare minimum values: the review and the rating. We split this into test and training datasets, and pass it through a binary count vectorizer, first to get a bag-of-words representation, second to get a bag-of-bigrams ( (1,2)-grams) ). Now we can start to fit various models to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words:\nClassifier Results: 99.19% training accuracy, 57.62% testing accuracy\nRegressor Results: 97.7% training accuracy, 58.36% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "hyperparameters = {'tol': 0.005, 'hidden_layer_sizes': (250, 100, 50), 'random_state': 1}\n",
    "def per(x): return(str(round(100*x,2))+'%')\n",
    "mlp = MLPClassifier(**hyperparameters)\n",
    "mlp.fit(train_bow, train_ratings)\n",
    "regressor = MLPRegressor(**hyperparameters)\n",
    "regressor.fit(train_bow, train_ratings)\n",
    "nn_classifier_bow_train = mlp.score(train_bow, train_ratings)\n",
    "nn_classifier_bow_test = mlp.score(test_bow, test_ratings)\n",
    "nn_regressor_bow_train = regressor.score(train_bow, train_ratings)\n",
    "nn_regressor_bow_test = regressor.score(test_bow, test_ratings)\n",
    "print('Bag-of-Words:')\n",
    "print('Classifier Results: '+per(nn_classifier_bow_train)+' training accuracy, '+per(nn_classifier_bow_test)+' testing accuracy')\n",
    "print('Regressor Results: '+per(nn_regressor_bow_train)+' training accuracy, '+per(nn_regressor_bow_test)+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Bigrams:\nClassifier Results: 99.0% training accuracy, 57.75% testing accuracy\nRegressor Results: 97.75% training accuracy, 58.07% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(**hyperparameters)\n",
    "regressor2 = MLPRegressor(**hyperparameters)\n",
    "mlp2.fit(train_bigram, train_ratings)\n",
    "regressor2.fit(train_bigram, train_ratings)\n",
    "nn_classifier_bigram_train = mlp2.score(train_bigram, train_ratings)\n",
    "nn_classifier_bigram_test = mlp2.score(test_bigram, test_ratings)\n",
    "nn_regressor_bigram_train = regressor2.score(train_bigram, train_ratings)\n",
    "nn_regressor_bigram_test = regressor2.score(test_bigram, test_ratings)\n",
    "print('Bag-of-Bigrams:')\n",
    "print('Classifier Results: '+per(nn_classifier_bigram_train)+' training accuracy, '+per(nn_classifier_bigram_test)+' testing accuracy')\n",
    "print('Regressor Results: '+per(nn_regressor_bigram_train)+' training accuracy, '+per(nn_regressor_bigram_test)+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Results:\nBOW Results: 60.73% training accuracy, 58.85% testing accuracy\nBigram Results: 60.83% training accuracy, 59.35% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC #As used by a blogger on Yelp review data \n",
    "svm = LinearSVC()\n",
    "svm2 = LinearSVC()\n",
    "svm.fit(train_bow, train_ratings)\n",
    "svm2.fit(train_bigram, train_ratings)\n",
    "svm_bow_train = svm.score(train_bow, train_ratings)\n",
    "svm_bow_test = svm.score(test_bow, test_ratings)\n",
    "svm_bigram_train = svm2.score(train_bigram, train_ratings)\n",
    "svm_bigram_test = svm2.score(test_bigram, test_ratings)\n",
    "print('SVM Results:')\n",
    "print('BOW Results: '+per(svm_bow_train)+' training accuracy, '+per(svm_bow_test)+' testing accuracy')\n",
    "print('Bigram Results: '+per(svm_bigram_train)+' training accuracy, '+per(svm_bigram_test)+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passive Aggressive Classifier\nBOW Results: 49.61% training accuracy, 48.31% testing accuracy\nBigram Results: 50.35% training accuracy, 49.36% testing accuracy\nPassive Aggressive Regressor\nBOW Results: -34.09% training accuracy, -29.9% testing accuracy\nBigram Results: -24.68% training accuracy, -24.68% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier, PassiveAggressiveRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pac = PassiveAggressiveClassifier()\n",
    "pac2 = PassiveAggressiveClassifier()\n",
    "par = PassiveAggressiveRegressor()\n",
    "par2 = PassiveAggressiveRegressor()\n",
    "pac.fit(train_bow, train_ratings)\n",
    "par.fit(train_bow, train_ratings)\n",
    "pac2.fit(train_bigram, train_ratings)\n",
    "par2.fit(train_bigram, train_ratings)\n",
    "pac_bow_train = pac.score(train_bow, train_ratings)\n",
    "pac_bow_test = pac.score(test_bow, test_ratings)\n",
    "pac_bigram_train = pac2.score(train_bigram, train_ratings)\n",
    "pac_bigram_test = pac2.score(test_bigram, test_ratings)\n",
    "par_bow_train = par.score(train_bow, train_ratings)\n",
    "par_bow_test = par.score(test_bow, test_ratings)\n",
    "par_bigram_train = par2.score(train_bigram, train_ratings)\n",
    "par_bigram_test = par2.score(test_bigram, test_ratings)\n",
    "print('Passive Aggressive Classifier')\n",
    "print('BOW Results: '+per(pac_bow_train)+' training accuracy, '+per(pac_bow_test)+' testing accuracy')\n",
    "print('Bigram Results: '+per(pac_bigram_train)+' training accuracy, '+per(pac_bigram_test)+' testing accuracy')\n",
    "print('Passive Aggressive Regressor')\n",
    "print('BOW Results: '+per(par_bow_train)+' training accuracy, '+per(par_bow_test)+' testing accuracy')\n",
    "print('Bigram Results: '+per(par_bigram_test)+' training accuracy, '+per(par_bigram_test)+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\nBOW: train:61.15%, test:59.06%\nBigram: train:61.38%, test:59.73%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(solver='sag', random_state=1, max_iter=200)\n",
    "logistic2 = LogisticRegression(solver='sag', random_state=1, max_iter=200)\n",
    "logistic.fit(train_bow, train_ratings)\n",
    "logistic2.fit(train_bigram, train_ratings)\n",
    "log_bow_train = logistic.score(train_bow, train_ratings)\n",
    "log_bow_test = logistic.score(test_bow, test_ratings)\n",
    "log_bigram_train = logistic2.score(train_bigram, train_ratings)\n",
    "log_bigram_test = logistic2.score(test_bigram, test_ratings)\n",
    "print('Logistic Regression:')\n",
    "print('BOW: train:'+per(log_bow_train)+', test:'+per(log_bow_test))\n",
    "print('Bigram: train:'+per(log_bigram_train)+', test:'+per(log_bigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\nBOW: train:99.97%, test:57.96%\nBigram: train:99.97%, test:58.69%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "rfc = RandomForestClassifier(random_state=1, n_estimators = 30) #n_estimators=10\n",
    "rfc2 = RandomForestClassifier(random_state=1, n_estimators = 30) #n_estimators=10\n",
    "rfc.fit(train_bow, train_ratings)\n",
    "rfc2.fit(train_bigram, train_ratings)\n",
    "rfc_bow_train = rfc.score(train_bow, train_ratings)\n",
    "rfc_bow_test = rfc.score(test_bow, test_ratings)\n",
    "rfc_bigram_train = rfc2.score(train_bigram, train_ratings)\n",
    "rfc_bigram_test = rfc2.score(test_bigram, test_ratings)\n",
    "print('Random Forest Classifier:')\n",
    "print('BOW: train:'+per(rfc_bow_train)+', test:'+per(rfc_bow_test))\n",
    "print('Bigram: train:'+per(rfc_bigram_train)+', test:'+per(rfc_bigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=1, n_estimators=30)\n",
    "rfr2 = RandomForestRegressor(random_state=1, n_estimators=30) #n_jobs=-1\n",
    "rfr.fit(train_bow, train_ratings)\n",
    "rfr2.fit(train_bigram, train_ratings)\n",
    "rfr_bow_train = rfr.score(train_bow, train_ratings)\n",
    "rfr_bow_test = rfr.score(test_bow, test_ratings)\n",
    "rfr_bigram_train = rfr2.score(train_bigram, train_ratings)\n",
    "rfr_bigram_test = rfr2.score(test_bigram, test_ratings)\n",
    "print('Random Forest Regressor:')\n",
    "print('BOW: train:'+per(rfr_bow_train)+', test:'+per(rfr_bow_test))\n",
    "print('Bigram: train:'+per(rfr_bigram_train)+', test:'+per(rfr_bigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier:\nBOW: train:99.99%, test:49.15%\nBigram: train:99.99%, test:49.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "dtc = DecisionTreeClassifier(random_state=1)\n",
    "dtc2 = DecisionTreeClassifier(random_state=1)\n",
    "dtc.fit(train_bow, train_ratings)\n",
    "dtc2.fit(train_bigram, train_ratings)\n",
    "dtc_bow_train = dtc.score(train_bow, train_ratings)\n",
    "dtc_bow_test = dtc.score(test_bow, test_ratings)\n",
    "dtc_bigram_train = dtc2.score(train_bigram, train_ratings)\n",
    "dtc_bigram_test = dtc2.score(test_bigram, test_ratings)\n",
    "print('Decision Tree Classifier:')\n",
    "print('BOW: train:'+per(dtc_bow_train)+', test:'+per(dtc_bow_test))\n",
    "print('Bigram: train:'+per(dtc_bigram_train)+', test:'+per(dtc_bigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "dtr2 = DecisionTreeRegressor()\n",
    "dtr.fit(train_bow, train_ratings)\n",
    "dtr2.fit(train_bigram, train_ratings)\n",
    "dtr_bow_train = dtr.score(train_bow, train_ratings)\n",
    "dtr_bow_test = dtr.score(test_bow, test_ratings)\n",
    "dtr_bigram_train = dtr2.score(train_bigram, train_ratings)\n",
    "dtr_bigram_test = dtr2.score(test_bigram, test_ratings)\n",
    "print('Decision Tree Regressor:')\n",
    "print('BOW: train:'+per(dtr_bow_train)+', test:'+per(dtr_bow_test))\n",
    "print('Bigram: train:'+per(dtr_bigram_train)+', test:'+per(dtr_bigram_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['NN_Classifier', 'NN_Regressor', 'SVM', 'PA_Classifier', 'PA_Regressor', 'Logistic', 'RF_Classifier', 'RF_Regressor', 'DT_Classifier', 'DT_Regressor']\n",
    "results = [['Bag-of-Words Training', 'Bag-of-Words Test', 'Bag-of-Bigrams Training', 'Bag-of-Bigrams Test'],\n",
    "           [nn_classifier_bow_train, nn_classifier_bow_test, nn_classifier_bigram_train, nn_classifier_bigram_test],\n",
    "           [nn_regressor_bow_train, nn_regressor_bow_test, nn_regressor_bigram_train, nn_regressor_bigram_test],\n",
    "           [svm_bow_train, svm_bow_test, svm_bigram_train, svm_bigram_test],\n",
    "           [pac_bow_train, pac_bow_test, pac_bigram_train, pac_bigram_test],\n",
    "           [par_bow_train, par_bow_test, par_bigram_train, par_bigram_test],\n",
    "           [log_bow_train, log_bow_test, log_bigram_train, log_bigram_test],\n",
    "           [rfc_bow_train, rfc_bow_test, rfc_bigram_train, rfc_bigram_test],\n",
    "           [rfr_bow_train, rfr_bow_test, rfr_bigram_train, rfr_bigram_test],\n",
    "           [dtc_bow_train, dtc_bow_test, dtc_bigram_train, dtc_bigram_test],\n",
    "           [dtr_bow_train, dtr_bow_test, dtr_bigram_train, dtr_bigram_test]]\n",
    "result_df = pd.DataFrame(results[1:], columns = results[0], index = models)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The SVM in question has a linear Kernal, and is based on \n",
    "http://www.developintelligence.com/blog/2017/03/predicting-yelp-star-ratings-review-text-python/\n",
    "\n",
    "Classifier's have an advantage in terms of accuracy because the output is restricted to one of the valid options \n",
    "(int 1,2,3,4,5) regressors can be any number, it can result in numbers way outside the possible range of valid \n",
    "outcomes and it can also result in numbers within the range but not exactly equal to any valid number (like floats\n",
    "between 1 and 2). Classifier accuracy can be judged based on whether or not they got the right value, with no partial\n",
    "credit assigned for getting values close to it. Since it must be one out of 5 possible options, there's a chance of \n",
    "getting it right or getting an above average accuracy even with a weak predictor model. For the regressors, accuracy\n",
    "is judged by R^2^ which even allows for a negative accuracy (as happened with the Passive Aggressive regressor). \n",
    "we can't avoid this or alter the 'score' metric easily as sklearn does not allow alternate error metrics or scores\n",
    "including Adjusted R^2^ or MSE or something similar. A weak predicting regressor would likely have a very low chance \n",
    "of getting anywhere near the correct value without being part of some gradient boosting system. Therefore, even though\n",
    "the classifiers outperformed the regressors accross-the-board in terms of score, if we take into account the scores and\n",
    "the a priori notion that regressors allow for the linearity of features and the relation between ratings, then we should\n",
    "accept a slightly lower score from a regressor as being comparable to a slightly higher score from a classifier on the \n",
    "same data. A notable comment to be made regarding the use of regressors is that the distance between the output labels\n",
    "matters. On Yelp rating data in particular, it would make the implicit assumption that the difference between a 1-star\n",
    "rating and a 2-star rating is the same as the difference between a 4-star and a 5-star rating and half the difference\n",
    "between a 2-star and 4-star rating and so on. This is not necessarily the case as Yelp never claims that it is a \n",
    "perfectly linear scale and Yelp users do not use it as such. \n",
    "\n",
    "More information on this subject can be gleaned from examilning the distribution of Yelp review ratings and restauarant\n",
    "ratings in our other EDA files, the ratings are not normally distributed, nor are they distributed in a way where one\n",
    "could assume a priori that they are a linear function. Review ratings have a negative skew, and are highly concentrated\n",
    "at the ratings 4 and 5. Restaurant ratings also have a negative skew, but since they are a mean of a sample of the \n",
    "previously discussed distribution, they are much more concentrated, with high frequencies for 3 and 4 and negligible \n",
    "frequency for all other values. \n",
    "\n",
    "Memory errors were encountered when using K-Nearest Neighbors Classifier and Regressor. I may try them again in a \n",
    "document where they are used alone, perhaps also using some Radius Neighbor methods. \n",
    "\n",
    "Now we try one more method, as used in a blog post on similar Yelp review-rating data: the Multinomial Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb2 = MultinomialNB()\n",
    "nb.fit(train_bow, train_ratings)\n",
    "nb2.fit(train_bigram, train_ratings)\n",
    "mnb_bow_train = nb.score(train_bow, train_ratings)\n",
    "mnb_bow_test = nb.score(test_bow, test_ratings)\n",
    "mnb_bigram_train = nb2.score(train_bigram, train_ratings)\n",
    "mnb_bigram_test = nb2.score(test_bigram, test_ratings)\n",
    "print(\"Multinomial Naive Bayes Results:\")\n",
    "print('BOW: train:'+per(mnb_bow_train)+', test:'+per(mnb_bow_test))\n",
    "print('Bigram: train:'+per(mnb_bigram_train)+', test:'+per(mnb_bigram_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the results from the Naive Bayes to the results of the other classifiers, above, we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
