{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use bag-of-words and bag-of-bigrams to predict ratings.\n",
    "BIA660D - Group 1: Alec Kulakowski\n",
    "Here we will explore some basic models applied niavely to simple 1-gram and 1,2-gram representations of the text. Based on performance in this regard, we can explore the most successful of the models in detail later, as well as different representations of the corpus, hyperparameter selection, and feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_rating                                          user_text  \\\n0            5  We booked Grand Vin as our brunch location to ...   \n1            4  Sooooo for date night it was his turn to pick ...   \n2            5  Adorable little wine bar with outdoor seating ...   \n\n  restaurant_name  restaurant_rating  restaurant_price  \\\n0       Grand Vin           3.994975               2.0   \n1       Grand Vin           4.000000               2.0   \n2       Grand Vin           3.994975               2.0   \n\n                             restaurant_type  \n0  ['Wine Bars', 'Italian', 'Cocktail Bars']  \n1  ['Wine Bars', 'Italian', 'Cocktail Bars']  \n2  ['Wine Bars', 'Italian', 'Cocktail Bars']  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../BIA660D_Group_1_Project/eda/hoboken_step1.csv')\n",
    "temp = data.head(3)\n",
    "reviews = data['user_text']\n",
    "ratings = data['user_rating']\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_reviews, test_reviews, train_ratings, test_ratings = train_test_split(reviews, ratings, test_size=0.15, random_state=1)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', binary=True, max_df=0.9, max_features=1000)\n",
    "vectorizer.fit(train_reviews)\n",
    "train_bow = vectorizer.transform(train_reviews)\n",
    "test_bow = vectorizer.transform(test_reviews)\n",
    "bigram_vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', binary=True, max_df=0.9, max_features=1000, ngram_range=(1,2))\n",
    "bigram_vectorizer.fit(train_reviews)\n",
    "train_bigram = bigram_vectorizer.transform(train_reviews)\n",
    "test_bigram = bigram_vectorizer.transform(test_reviews)\n",
    "del data, reviews, ratings, vectorizer, bigram_vectorizer\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we import the relevant data and examine it. We then select the bare minimum values: the review and the rating. We split this into test and training datasets, and pass it through a binary count vectorizer, first to get a bag-of-words representation, second to get a bag-of-bigrams ( (1,2)-grams) ). Now we can start to fit various models to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words:\nClassifier Results: 99.19% training accuracy, 57.62% testing accuracy\nRegressor Results: 97.7% training accuracy, 58.36% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "hyperparameters = {'tol': 0.005, 'hidden_layer_sizes': (250, 100, 50), 'random_state': 1}\n",
    "def per(x): return(str(round(100*x,2))+'%')\n",
    "mlp = MLPClassifier(**hyperparameters)\n",
    "mlp.fit(train_bow, train_ratings)\n",
    "regressor = MLPRegressor(**hyperparameters)\n",
    "regressor.fit(train_bow, train_ratings)\n",
    "nn_classifier_bow_train = mlp.score(train_bow, train_ratings)\n",
    "nn_classifier_bow_test = mlp.score(test_bow, test_ratings)\n",
    "nn_regressor_bow_train = regressor.score(train_bow, train_ratings)\n",
    "nn_regressor_bow_test = regressor.score(test_bow, test_ratings)\n",
    "print('Bag-of-Words:')\n",
    "print('Classifier Results: '+per(nn_classifier_bow_train)+' training accuracy, '+per(nn_classifier_bow_test)+' testing accuracy')\n",
    "print('Regressor Results: '+per(nn_regressor_bow_train)+' training accuracy, '+per(nn_regressor_bow_test)+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Bigrams:\nClassifier Results: 99.0% training accuracy, 57.75% testing accuracy\nRegressor Results: 97.75% training accuracy, 58.07% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(**hyperparameters)\n",
    "regressor2 = MLPRegressor(**hyperparameters)\n",
    "mlp2.fit(train_bigram, train_ratings)\n",
    "regressor2.fit(train_bigram, train_ratings)\n",
    "nn_classifier_bigram_train = mlp2.score(train_bigram, train_ratings)\n",
    "nn_classifier_bigram_test = mlp2.score(test_bigram, test_ratings)\n",
    "nn_regressor_bigram_train = regressor2.score(train_bigram, train_ratings)\n",
    "nn_regressor_bigram_test = regressor2.score(test_bigram, test_ratings)\n",
    "print('Bag-of-Bigrams:')\n",
    "print('Classifier Results: '+per(nn_classifier_bigram_train)+' training accuracy, '+per(nn_classifier_bigram_test)+' testing accuracy')\n",
    "print('Regressor Results: '+per(nn_regressor_bigram_train)+' training accuracy, '+per(nn_regressor_bigram_test)+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Results:\nBOW Results: 60.73% training accuracy, 58.85% testing accuracy\nBigram Results: 60.83% training accuracy, 59.35% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC #As used by a blogger on Yelp review data \n",
    "svm = LinearSVC()\n",
    "svm2 = LinearSVC()\n",
    "svm.fit(train_bow, train_ratings)\n",
    "svm2.fit(train_bigram, train_ratings)\n",
    "svm_bow_train = svm.score(train_bow, train_ratings)\n",
    "svm_bow_test = svm.score(test_bow, test_ratings)\n",
    "svm_bigram_train = svm2.score(train_bigram, train_ratings)\n",
    "svm_bigram_test = svm2.score(test_bigram, test_ratings)\n",
    "print('SVM Results:')\n",
    "print('BOW Results: '+per(svm_bow_train)+' training accuracy, '+per(svm_bow_test)+' testing accuracy')\n",
    "print('Bigram Results: '+per(svm_bigram_train)+' training accuracy, '+per(svm_bigram_test)+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passive Aggressive Classifier\nBOW Results: 49.6% training accuracy, 48.74% testing accuracy\nBigram Results: 50.4% training accuracy, 48.86% testing accuracy\nPassive Aggressive Regressor\nBOW Results: -13.26% training accuracy, -14.82% testing accuracy\nBigram Results: -30.71% training accuracy, -30.71% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier, PassiveAggressiveRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pac = PassiveAggressiveClassifier()\n",
    "pac2 = PassiveAggressiveClassifier()\n",
    "par = PassiveAggressiveRegressor()\n",
    "par2 = PassiveAggressiveRegressor()\n",
    "pac.fit(train_bow, train_ratings)\n",
    "par.fit(train_bow, train_ratings)\n",
    "pac2.fit(train_bigram, train_ratings)\n",
    "par2.fit(train_bigram, train_ratings)\n",
    "pac_bow_train = pac.score(train_bow, train_ratings)\n",
    "pac_bow_test = pac.score(test_bow, test_ratings)\n",
    "pac_bigram_train = pac2.score(train_bigram, train_ratings)\n",
    "pac_bigram_test = pac2.score(test_bigram, test_ratings)\n",
    "par_bow_train = par.score(train_bow, train_ratings)\n",
    "par_bow_test = par.score(test_bow, test_ratings)\n",
    "par_bigram_train = par2.score(train_bigram, train_ratings)\n",
    "par_bigram_test = par2.score(test_bigram, test_ratings)\n",
    "print('Passive Aggressive Classifier')\n",
    "print('BOW Results: '+per(pac_bow_train)+' training accuracy, '+per(pac_bow_test)+' testing accuracy')\n",
    "print('Bigram Results: '+per(pac_bigram_train)+' training accuracy, '+per(pac_bigram_test)+' testing accuracy')\n",
    "print('Passive Aggressive Regressor')\n",
    "print('BOW Results: '+per(par_bow_train)+' training accuracy, '+per(par_bow_test)+' testing accuracy')\n",
    "print('Bigram Results: '+per(par_bigram_test)+' training accuracy, '+per(par_bigram_test)+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\nBOW: train:61.15%, test:59.06%\nBigram: train:61.38%, test:59.73%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(solver='sag', random_state=1, max_iter=200)\n",
    "logistic2 = LogisticRegression(solver='sag', random_state=1, max_iter=200)\n",
    "logistic.fit(train_bow, train_ratings)\n",
    "logistic2.fit(train_bigram, train_ratings)\n",
    "log_bow_train = logistic.score(train_bow, train_ratings)\n",
    "log_bow_test = logistic.score(test_bow, test_ratings)\n",
    "log_bigram_train = logistic2.score(train_bigram, train_ratings)\n",
    "log_bigram_test = logistic2.score(test_bigram, test_ratings)\n",
    "print('Logistic Regression:')\n",
    "print('BOW: train:'+per(log_bow_train)+', test:'+per(log_bow_test))\n",
    "print('Bigram: train:'+per(log_bigram_train)+', test:'+per(log_bigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\nBOW: train:99.97%, test:57.96%\nBigram: train:99.97%, test:58.69%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "rfc = RandomForestClassifier(random_state=1, n_estimators = 30) #n_estimators=10\n",
    "rfc2 = RandomForestClassifier(random_state=1, n_estimators = 30) #n_estimators=10\n",
    "rfc.fit(train_bow, train_ratings)\n",
    "rfc2.fit(train_bigram, train_ratings)\n",
    "rfc_bow_train = rfc.score(train_bow, train_ratings)\n",
    "rfc_bow_test = rfc.score(test_bow, test_ratings)\n",
    "rfc_bigram_train = rfc2.score(train_bigram, train_ratings)\n",
    "rfc_bigram_test = rfc2.score(test_bigram, test_ratings)\n",
    "print('Random Forest Classifier:')\n",
    "print('BOW: train:'+per(rfc_bow_train)+', test:'+per(rfc_bow_test))\n",
    "print('Bigram: train:'+per(rfc_bigram_train)+', test:'+per(rfc_bigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor:\nBOW: train:93.4%, test:56.52%\nBigram: train:93.42%, test:56.64%\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=1, n_estimators=30)\n",
    "rfr2 = RandomForestRegressor(random_state=1, n_estimators=30) #n_jobs=-1\n",
    "rfr.fit(train_bow, train_ratings)\n",
    "rfr2.fit(train_bigram, train_ratings)\n",
    "rfr_bow_train = rfr.score(train_bow, train_ratings)\n",
    "rfr_bow_test = rfr.score(test_bow, test_ratings)\n",
    "rfr_bigram_train = rfr2.score(train_bigram, train_ratings)\n",
    "rfr_bigram_test = rfr2.score(test_bigram, test_ratings)\n",
    "print('Random Forest Regressor:')\n",
    "print('BOW: train:'+per(rfr_bow_train)+', test:'+per(rfr_bow_test))\n",
    "print('Bigram: train:'+per(rfr_bigram_train)+', test:'+per(rfr_bigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier:\nBOW: train:99.99%, test:49.15%\nBigram: train:99.99%, test:49.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "dtc = DecisionTreeClassifier(random_state=1)\n",
    "dtc2 = DecisionTreeClassifier(random_state=1)\n",
    "dtc.fit(train_bow, train_ratings)\n",
    "dtc2.fit(train_bigram, train_ratings)\n",
    "dtc_bow_train = dtc.score(train_bow, train_ratings)\n",
    "dtc_bow_test = dtc.score(test_bow, test_ratings)\n",
    "dtc_bigram_train = dtc2.score(train_bigram, train_ratings)\n",
    "dtc_bigram_test = dtc2.score(test_bigram, test_ratings)\n",
    "print('Decision Tree Classifier:')\n",
    "print('BOW: train:'+per(dtc_bow_train)+', test:'+per(dtc_bow_test))\n",
    "print('Bigram: train:'+per(dtc_bigram_train)+', test:'+per(dtc_bigram_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor:\nBOW: train:99.99%, test:18.1%\nBigram: train:99.99%, test:17.26%\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "dtr2 = DecisionTreeRegressor()\n",
    "dtr.fit(train_bow, train_ratings)\n",
    "dtr2.fit(train_bigram, train_ratings)\n",
    "dtr_bow_train = dtr.score(train_bow, train_ratings)\n",
    "dtr_bow_test = dtr.score(test_bow, test_ratings)\n",
    "dtr_bigram_train = dtr2.score(train_bigram, train_ratings)\n",
    "dtr_bigram_test = dtr2.score(test_bigram, test_ratings)\n",
    "print('Decision Tree Regressor:')\n",
    "print('BOW: train:'+per(dtr_bow_train)+', test:'+per(dtr_bow_test))\n",
    "print('Bigram: train:'+per(dtr_bigram_train)+', test:'+per(dtr_bigram_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bag-of-Words Training</th>\n",
       "      <th>Bag-of-Words Test</th>\n",
       "      <th>Bag-of-Bigrams Training</th>\n",
       "      <th>Bag-of-Bigrams Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN_Classifier</th>\n",
       "      <td>0.991911</td>\n",
       "      <td>0.576215</td>\n",
       "      <td>0.990019</td>\n",
       "      <td>0.577466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN_Regressor</th>\n",
       "      <td>0.976990</td>\n",
       "      <td>0.583614</td>\n",
       "      <td>0.977513</td>\n",
       "      <td>0.580725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.607263</td>\n",
       "      <td>0.588456</td>\n",
       "      <td>0.608304</td>\n",
       "      <td>0.593549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA_Classifier</th>\n",
       "      <td>0.495987</td>\n",
       "      <td>0.487402</td>\n",
       "      <td>0.504029</td>\n",
       "      <td>0.488563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA_Regressor</th>\n",
       "      <td>-0.132630</td>\n",
       "      <td>-0.148169</td>\n",
       "      <td>-0.304142</td>\n",
       "      <td>-0.307087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.611489</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>0.613759</td>\n",
       "      <td>0.597302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF_Classifier</th>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.579610</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.586937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF_Regressor</th>\n",
       "      <td>0.933987</td>\n",
       "      <td>0.565248</td>\n",
       "      <td>0.934165</td>\n",
       "      <td>0.566366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT_Classifier</th>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.491512</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.492048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT_Regressor</th>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.180993</td>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.172578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bag-of-Words Training</th>\n",
       "      <th>Bag-of-Words Test</th>\n",
       "      <th>Bag-of-Bigrams Training</th>\n",
       "      <th>Bag-of-Bigrams Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN_Classifier</th>\n",
       "      <td>0.991911</td>\n",
       "      <td>0.576215</td>\n",
       "      <td>0.990019</td>\n",
       "      <td>0.577466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN_Regressor</th>\n",
       "      <td>0.976990</td>\n",
       "      <td>0.583614</td>\n",
       "      <td>0.977513</td>\n",
       "      <td>0.580725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.607263</td>\n",
       "      <td>0.588456</td>\n",
       "      <td>0.608304</td>\n",
       "      <td>0.593549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA_Classifier</th>\n",
       "      <td>0.495987</td>\n",
       "      <td>0.487402</td>\n",
       "      <td>0.504029</td>\n",
       "      <td>0.488563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA_Regressor</th>\n",
       "      <td>-0.132630</td>\n",
       "      <td>-0.148169</td>\n",
       "      <td>-0.304142</td>\n",
       "      <td>-0.307087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.611489</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>0.613759</td>\n",
       "      <td>0.597302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF_Classifier</th>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.579610</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.586937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF_Regressor</th>\n",
       "      <td>0.933987</td>\n",
       "      <td>0.565248</td>\n",
       "      <td>0.934165</td>\n",
       "      <td>0.566366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT_Classifier</th>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.491512</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.492048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT_Regressor</th>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.180993</td>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.172578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['NN_Classifier', 'NN_Regressor', 'SVM', 'PA_Classifier', 'PA_Regressor', 'Logistic', 'RF_Classifier', 'RF_Regressor', 'DT_Classifier', 'DT_Regressor']\n",
    "results = [['Bag-of-Words Training', 'Bag-of-Words Test', 'Bag-of-Bigrams Training', 'Bag-of-Bigrams Test'],\n",
    "           [nn_classifier_bow_train, nn_classifier_bow_test, nn_classifier_bigram_train, nn_classifier_bigram_test],\n",
    "           [nn_regressor_bow_train, nn_regressor_bow_test, nn_regressor_bigram_train, nn_regressor_bigram_test],\n",
    "           [svm_bow_train, svm_bow_test, svm_bigram_train, svm_bigram_test],\n",
    "           [pac_bow_train, pac_bow_test, pac_bigram_train, pac_bigram_test],\n",
    "           [par_bow_train, par_bow_test, par_bigram_train, par_bigram_test],\n",
    "           [log_bow_train, log_bow_test, log_bigram_train, log_bigram_test],\n",
    "           [rfc_bow_train, rfc_bow_test, rfc_bigram_train, rfc_bigram_test],\n",
    "           [rfr_bow_train, rfr_bow_test, rfr_bigram_train, rfr_bigram_test],\n",
    "           [dtc_bow_train, dtc_bow_test, dtc_bigram_train, dtc_bigram_test],\n",
    "           [dtr_bow_train, dtr_bow_test, dtr_bigram_train, dtr_bigram_test]]\n",
    "result_df = pd.DataFrame(results[1:], columns = results[0], index = models)\n",
    "# result_df = pd.DataFrame.from_csv('../BIA660D_Group_1_Project/prediction/bagofwords.csv') # in case of errors use this\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The SVM in question has a linear Kernal, and is based on \n",
    "http://www.developintelligence.com/blog/2017/03/predicting-yelp-star-ratings-review-text-python/\n",
    "\n",
    "Classifier's have an advantage in terms of accuracy because the output is restricted to one of the valid options \n",
    "(int 1,2,3,4,5) regressors can be any number, it can result in numbers way outside the possible range of valid \n",
    "outcomes and it can also result in numbers within the range but not exactly equal to any valid number (like floats\n",
    "between 1 and 2). Classifier accuracy can be judged based on whether or not they got the right value, with no partial\n",
    "credit assigned for getting values close to it. Since it must be one out of 5 possible options, there's a chance of \n",
    "getting it right or getting an above average accuracy even with a weak predictor model. For the regressors, accuracy\n",
    "is judged by R^2^ which even allows for a negative accuracy (as happened with the Passive Aggressive regressor). \n",
    "we can't avoid this or alter the 'score' metric easily as sklearn does not allow alternate error metrics or scores\n",
    "including Adjusted R^2^ or MSE or something similar. A weak predicting regressor would likely have a very low chance \n",
    "of getting anywhere near the correct value without being part of some gradient boosting system. Therefore, even though\n",
    "the classifiers outperformed the regressors accross-the-board in terms of score, if we take into account the scores and\n",
    "the a priori notion that regressors allow for the linearity of features and the relation between ratings, then we should\n",
    "accept a slightly lower score from a regressor as being comparable to a slightly higher score from a classifier on the \n",
    "same data. A notable comment to be made regarding the use of regressors is that the distance between the output labels\n",
    "matters. On Yelp rating data in particular, it would make the implicit assumption that the difference between a 1-star\n",
    "rating and a 2-star rating is the same as the difference between a 4-star and a 5-star rating and half the difference\n",
    "between a 2-star and 4-star rating and so on. This is not necessarily the case as Yelp never claims that it is a \n",
    "perfectly linear scale and Yelp users do not use it as such. \n",
    "\n",
    "More information on this subject can be gleaned from examilning the distribution of Yelp review ratings and restauarant\n",
    "ratings in our other EDA files, the ratings are not normally distributed, nor are they distributed in a way where one\n",
    "could assume a priori that they are a linear function. Review ratings have a negative skew, and are highly concentrated\n",
    "at the ratings 4 and 5. Restaurant ratings also have a negative skew, but since they are a mean of a sample of the \n",
    "previously discussed distribution, they are much more concentrated, with high frequencies for 3 and 4 and negligible \n",
    "frequency for all other values. \n",
    "\n",
    "Memory errors were encountered when using K-Nearest Neighbors Classifier and Regressor. I may try them again in a \n",
    "document where they are used alone, perhaps also using some Radius Neighbor methods. \n",
    "\n",
    "Now we try one more method, as used in a blog post on similar Yelp review-rating data: the Multinomial Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Results:\nBOW: train:58.04%, test:57.91%\nBigram: train:58.08%, test:57.73%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb2 = MultinomialNB()\n",
    "nb.fit(train_bow, train_ratings)\n",
    "nb2.fit(train_bigram, train_ratings)\n",
    "mnb_bow_train = nb.score(train_bow, train_ratings)\n",
    "mnb_bow_test = nb.score(test_bow, test_ratings)\n",
    "mnb_bigram_train = nb2.score(train_bigram, train_ratings)\n",
    "mnb_bigram_test = nb2.score(test_bigram, test_ratings)\n",
    "print(\"Multinomial Naive Bayes Results:\")\n",
    "print('BOW: train: '+per(mnb_bow_train)+', test: '+per(mnb_bow_test))\n",
    "print('Bigram: train: '+per(mnb_bigram_train)+', test: '+per(mnb_bigram_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the results from the Naive Bayes to the results of the other classifiers above, we can see that while the test scores are comparable to the best of those that were reviewed, the training accuracy was signifigantly lower (similar to the behavior of the linear-kernal support vector machine, passive aggressive methods, and the logistic regression). \n",
    "\n",
    "Since the training and test score were so near to each other (as we had with the SVM, PA, and Logistic models) that's a pretty good assurance that it's not overfitting the training data, however, the low score during training serves as a bound for maximum expected accuracy on the test data. Thus, even using the best of these options, the most accuracy we can hope for would be roughly 61% in the best case scenario. With tuning and exploration of alternative hyperparameters that training accuracy could potentially be improved, and the test accuracy would likely follow close behind. \n",
    "\n",
    "If, however, we chose a model with a greater training-test disparity such as the random forest or neural networks, (certainly not the decision trees, which had a large disparity between training and test scores but also had some of the worst test accuracy metrics overall) our focus can be on preventing overfitting. Random forests and neural networks also typically have many more tunable hyperparameters than the alternative models which were tested. This means that the number of possible configurations is greater and since each configuration could be the one that overfits less or learns more valuable information about the data, one is a priori tempted to think that the chance of finding a successful model is better between these two options. The no free lunch theorem tells us that just because these models are more tunable, they aren't necessarily going to work better then the alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
